hist(dataset$rad)
hist(dataset$tax)
hist(dataset$ptratio)
hist(dataset$black)
# par(mfrow=c(1,2))
hist(dataset$lstat)
hist(dataset$medv)
graphics.off()
ordered_data <- dataset
ordered_data[["crim"]] <- ordered(cut(dataset[["crim"]], c(0,10,20,90)), labels = c("Low", "Moderate", "High"))
ordered_data[["zn"]] <- ordered(cut(dataset[["zn"]], c(0,30,60,100)), labels = c("Small", "Medium", "Large"))
ordered_data[["indus"]] <- ordered(cut(dataset[["indus"]], c(0,10,20,30)), labels = c("Small", "Medium", "Large"))
ordered_data[["chas"]] <- NULL
ordered_data[["nox"]] <- ordered(cut(dataset[["nox"]], c(0.3,0.5,0.7,0.9)), labels = c("Low", "Medium", "High"))
ordered_data[["rm"]] <- ordered(cut(dataset[["rm"]], c(3,5,7,9)), labels = c("Low", "Medium", "High"))
ordered_data[["age"]] <- ordered(cut(dataset[["age"]], c(0,25,60,100)), labels = c("New", "Fair", "Old"))
ordered_data[["dis"]] <- ordered(cut(dataset[["dis"]], c(3,5,7,9)), labels = c("Close", "Fair", "Far"))
ordered_data[["rad"]] <- ordered(cut(dataset[["rad"]], c(0,10,24)), labels = c("Accessible", "Not Accessible"))
ordered_data[["tax"]] <- ordered(cut(dataset[["tax"]], c(0,300,500,900)), labels = c("Low", "Medium", "High"))
ordered_data[["ptratio"]] <- ordered(cut(dataset[["ptratio"]], c(0,16,20,24)), labels = c("Low", "Medium", "High"))
ordered_data[["black"]] <- ordered(cut(dataset[["black"]], c(0,100,200,400)), labels = c("Low",'Medium', "High"))
ordered_data[["lstat"]] <- ordered(cut(dataset[["lstat"]], c(0,12,24,40)), labels = c("Low", "Medium", "High"))
ordered_data[["medv"]] <- ordered(cut(dataset[["medv"]], c(0,17,34,51)), labels = c("Low", "Medium", "High"))
#Create binary incidence matrix
bin_inc_matrix <- as(ordered_data, "transactions")
summary(bin_inc_matrix)
#Visualise data
itemFrequencyPlot(bin_inc_matrix, support = 0.15, cex.names = 0.8)
title("Item-Frequency Plot")
# Apply Apriori algorithm
rules  <- apriori(bin_inc_matrix, parameter = list(support = 0.10, confidence = 0.75))
# Apply Apriori algorithm
rules  <- apriori(bin_inc_matrix, parameter = list(support = 0.07, confidence = 0.75))
# Apply Apriori algorithm
rules  <- apriori(bin_inc_matrix, parameter = list(support = 0.06, confidence = 0.75))
# Apply Apriori algorithm
rules  <- apriori(bin_inc_matrix, parameter = list(support = 0.05, confidence = 0.75))
# Apply Apriori algorithm
rules  <- apriori(bin_inc_matrix, parameter = list(support = 0.10, confidence = 0.75))
# Display summary of the rules
summary(rules)
#Visualise data
itemFrequencyPlot(bin_inc_matrix, support = 0.15, cex.names = 0.8)
title("Item-Frequency Plot")
graphics.off()
# Apply Apriori algorithm
rules  <- apriori(bin_inc_matrix, parameter = list(support = 0.10, confidence = 0.75))
# Display summary of the rules
summary(rules)
inspect(head(subset(rules, subset=rhs %in% "crim=Low"),5,by="confidence"))
inspect(head(subset(rules, subset=rhs %in% "crim=Low"),5,by="lift"))
inspect(head(subset(rules, subset=rhs %in% "dis=Close"),5,by="confidence"))
inspect(head(subset(rules, subset=rhs %in% "dis=Close"),5,by="lift"))
# Apply Apriori algorithm
rules  <- apriori(bin_inc_matrix, parameter = list(support = 0.05, confidence = 0.75))
# Display summary of the rules
summary(rules)
inspect(head(subset(rules, subset=rhs %in% "crim=Low"),5,by="confidence"))
inspect(head(subset(rules, subset=rhs %in% "crim=Low"),5,by="lift"))
inspect(head(subset(rules, subset=rhs %in% "dis=Close"),5,by="confidence"))
inspect(head(subset(rules, subset=rhs %in% "dis=Close"),5,by="lift"))
inspect(head(subset(rules, subset=rhs %in% "ptratio=Low"),5,by="confidence"))
#Split data into train and test
set.seed(6703)
split = sample(1:nrow(Boston), size=nrow(Boston)*0.75)
train_data = Boston[split,]
test_data = Boston[-split,]
#Create model
linear_model <- lm(ptratio~.,train_data)
summary(linear_model)
#Make predictions
prediction = predict.lm(linear_model, newdata = test_data)
#Calculate error
error = mean((test_data$ptratio - prediction)^2)
error
#Make predictions
prediction = predict.lm(linear_model, newdata = test_data)
#Calculate error
error = mean((test_data$ptratio - prediction)^2)
error
#Plot significant variables
scatter.smooth(Boston$ptratio,Boston$zn)
scatter.smooth(Boston$ptratio,Boston$nox)
scatter.smooth(Boston$ptratio,Boston$rad)
scatter.smooth(Boston$ptratio,Boston$medv)
#Plot significant variables
par(mfrow = (2,2))
scatter.smooth(Boston$ptratio,Boston$zn)
#Plot significant variables
par(mfrow = c(2,2))
scatter.smooth(Boston$ptratio,Boston$zn)
scatter.smooth(Boston$ptratio,Boston$nox)
scatter.smooth(Boston$ptratio,Boston$rad)
scatter.smooth(Boston$ptratio,Boston$medv)
rm(list = ls())
#### Load Package
library(ISLR)
library(ElemStatLearn)
library(DataExplorer)
library(rpart)
library(rpart.plot)
library(arules)
#Load data
dataset = as.data.frame(marketing)
#Add a class column
N = dim(dataset)[1]
Class = sample(c(0,1), N,replace = T)
dataset["Class"] = Class
#Remove observations with NA values
dataset <- dataset[complete.cases(dataset),]
#Categorise variables
dataset[["Income"]] <- ordered(cut(dataset[["Income"]], c(0,1,2,3,4,5,6,7,8,9)), labels = c("Less than $10,000","$10,000 to $14,999", "$15,000 to $19,999", "$20,000 to $24,999", "$25,000 to $29,999","$30,000 to $39,999","$40,000 to $49,999", "$50,000 to $74,999", "$75,000 or more"))
dataset[["Sex"]] <- ordered(cut(dataset[["Sex"]], c(0,1,2)), labels = c("Male", "Female"))
dataset[["Marital"]] <- ordered(cut(dataset[["Marital"]], c(0,1,2,3,4,5)), labels = c("Married" ,"Living together, not married", "Divorced or separated","Widowed","Single, never married"))
dataset[["Age"]] <- ordered(cut(dataset[["Age"]], c(0,3,5,7)), labels = c("14 thru 34", "35 thru 54", "55 and over"))
dataset[["Edu"]] <- ordered(cut(dataset[["Edu"]], c(0,1,2,3,4,5,6)), labels = c("Grade 8 or less", "Grades 9 to 11","Graduated high school","1 to 3 years of college","College graduate","Grad Study"))
dataset[["Occupation"]] <- ordered(cut(dataset[["Occupation"]], c(0,1,2,3,4,5,6,7,8,9)), labels = c("Professional/Managerial","Sales Worker","Factory Worker/Laborer/Driver","Clerical/Service Worker","Homemaker", "Student, HS or College","Military", "Retired", "Unemployed"))
dataset[["Lived"]] <- ordered(cut(dataset[["Lived"]], c(0,1,2,3,4,5)), labels = c("Less than one year",  "One to three years",  "Four to six years",  "Seven to ten years","More than ten years"))
dataset[["Dual_Income"]] <- ordered(cut(dataset[["Dual_Income"]], c(0,1,2,3)), labels = c("Not Married", "Yes", "No"))
dataset[["Household"]] <- ordered(cut(dataset[["Household"]], c(0,1,2,3,4,5,6,7,8,9)), labels = c("One", "Two", "Three", "Four", "Five", "Six","Seven","Eight", "Nine or more"))
dataset[["Householdu18"]] <- ordered(cut(dataset[["Householdu18"]], c(-1,0,1,2,3,4,5,6,7,8,9)), labels = c("None", "One", "Two", "Three", "Four", "Five", "Six","Seven","Eight", "Nine or more"))
dataset[["Status"]] <- ordered(cut(dataset[["Status"]], c(0,1,2,3)), labels = c("Own", "Rent", "Live with Parents/Family"))
dataset[["Home_Type"]] <- ordered(cut(dataset[["Home_Type"]], c(0,1,2,3,4,5)), labels = c("House","Condominium","Apartment", "Mobile Home","Other"))
dataset[["Ethnic"]] <- ordered(cut(dataset[["Ethnic"]], c(0,1,2,3,4,5,6,7,8)), labels = c("American Indian","Asian", "Black","East Indian","Hispanic","Pacific Islander","White","Other"))
dataset[["Language"]] <- ordered(cut(dataset[["Language"]], c(0,1,2,3)), labels = c("English", "Spanish", "Other"))
#Creating training dataset and reference sample
dataset$Class = 1
train_data = dataset
ref_sample = dataset
for(i in 1:ncol(ref_sample)){
ref_sample[,i] = sample(ref_sample[,i], nrow(ref_sample), replace=TRUE)
}
ref_sample$Class = 0
combined_df = rbind(ref_sample, train_data)
for(i in 1:ncol(combined_df)){
combined_df[,i] = as.factor(as.character(combined_df[,i]))
}
#Creating a classification tree on the combined dataframe
tree_model <- rpart.control(maxdepth = 4, minsplit = 5, xval = 10, cp = 0)
tree <- rpart(Class~., data = combined_df, method = "class", control = tree_model)
#Plot tree
x11()
prp(tree, type = 4, extra = 101, leaf.round = 1, box.palette = "BuGn", nn=TRUE, main=" Classification Tree on combined dataset")
1
1
1
train_data = dataset
ref_sample = dataset
for(i in 1:ncol(ref_sample)){
ref_sample[,i] = sample(ref_sample[,i], nrow(ref_sample), replace=TRUE)
}
ref_sample$Class = 0
combined_df = rbind(ref_sample, train_data)
for(i in 1:ncol(combined_df)){
combined_df[,i] = as.factor(as.character(combined_df[,i]))
}
#Creating a classification tree on the combined dataframe
tree_model <- rpart.control(maxdepth = 4, minsplit = 5, xval = 10, cp = 0)
tree <- rpart(Class~., data = combined_df, method = "class", control = tree_model)
#Plot tree
x11()
prp(tree, type = 4, extra = 101, leaf.round = 1, box.palette = "BuGn", nn=TRUE, main=" Classification Tree on combined dataset")
marketingï¼Ÿ
marketing?
?
=d
?marketing
View(combined_df)
prp(tree, type = 4, extra = 101, leaf.round = 1, box.palette = "BuGn", nn=TRUE, main=" Classification Tree on combined dataset")
#Plot tree
x11()
#Plot tree
x11()
(tree, type = 4, extra = 101, leaf.round = 1, box.palette = "BuGn", nn=TRUE, main=" Classification Tree on combined dataset")
prp(tree, type = 4, extra = 101, leaf.round = 1, box.palette = "BuGn", nn=TRUE, main=" Classification Tree on combined dataset")
rm(list = ls())
#### Load Package
library(ISLR)
library(ElemStatLearn)
library(DataExplorer)
library(rpart)
library(rpart.plot)
library(arules)
#Load data
dataset = as.data.frame(marketing)
#Add a class column
N = dim(dataset)[1]
Class = sample(c(0,1), N,replace = T)
dataset["Class"] = Class
#Remove observations with NA values
dataset <- dataset[complete.cases(dataset),]
#Categorise variables
dataset[["Income"]] <- ordered(cut(dataset[["Income"]], c(0,1,2,3,4,5,6,7,8,9)), labels = c("Less than $10,000","$10,000 to $14,999", "$15,000 to $19,999", "$20,000 to $24,999", "$25,000 to $29,999","$30,000 to $39,999","$40,000 to $49,999", "$50,000 to $74,999", "$75,000 or more"))
dataset[["Sex"]] <- ordered(cut(dataset[["Sex"]], c(0,1,2)), labels = c("Male", "Female"))
dataset[["Marital"]] <- ordered(cut(dataset[["Marital"]], c(0,1,2,3,4,5)), labels = c("Married" ,"Living together, not married", "Divorced or separated","Widowed","Single, never married"))
dataset[["Age"]] <- ordered(cut(dataset[["Age"]], c(0,3,5,7)), labels = c("14 thru 34", "35 thru 54", "55 and over"))
dataset[["Edu"]] <- ordered(cut(dataset[["Edu"]], c(0,1,2,3,4,5,6)), labels = c("Grade 8 or less", "Grades 9 to 11","Graduated high school","1 to 3 years of college","College graduate","Grad Study"))
dataset[["Occupation"]] <- ordered(cut(dataset[["Occupation"]], c(0,1,2,3,4,5,6,7,8,9)), labels = c("Professional/Managerial","Sales Worker","Factory Worker/Laborer/Driver","Clerical/Service Worker","Homemaker", "Student, HS or College","Military", "Retired", "Unemployed"))
dataset[["Lived"]] <- ordered(cut(dataset[["Lived"]], c(0,1,2,3,4,5)), labels = c("Less than one year",  "One to three years",  "Four to six years",  "Seven to ten years","More than ten years"))
dataset[["Dual_Income"]] <- ordered(cut(dataset[["Dual_Income"]], c(0,1,2,3)), labels = c("Not Married", "Yes", "No"))
dataset[["Household"]] <- ordered(cut(dataset[["Household"]], c(0,1,2,3,4,5,6,7,8,9)), labels = c("One", "Two", "Three", "Four", "Five", "Six","Seven","Eight", "Nine or more"))
dataset[["Householdu18"]] <- ordered(cut(dataset[["Householdu18"]], c(-1,0,1,2,3,4,5,6,7,8,9)), labels = c("None", "One", "Two", "Three", "Four", "Five", "Six","Seven","Eight", "Nine or more"))
dataset[["Status"]] <- ordered(cut(dataset[["Status"]], c(0,1,2,3)), labels = c("Own", "Rent", "Live with Parents/Family"))
dataset[["Home_Type"]] <- ordered(cut(dataset[["Home_Type"]], c(0,1,2,3,4,5)), labels = c("House","Condominium","Apartment", "Mobile Home","Other"))
dataset[["Ethnic"]] <- ordered(cut(dataset[["Ethnic"]], c(0,1,2,3,4,5,6,7,8)), labels = c("American Indian","Asian", "Black","East Indian","Hispanic","Pacific Islander","White","Other"))
dataset[["Language"]] <- ordered(cut(dataset[["Language"]], c(0,1,2,3)), labels = c("English", "Spanish", "Other"))
#Creating training dataset and reference sample
dataset$Class = 1
train_data = dataset
ref_sample = dataset
for(i in 1:ncol(ref_sample)){
ref_sample[,i] = sample(ref_sample[,i], nrow(ref_sample), replace=TRUE)
}
ref_sample$Class = 0
combined_df = rbind(ref_sample, train_data)
for(i in 1:ncol(combined_df)){
combined_df[,i] = as.factor(as.character(combined_df[,i]))
}
#Creating a classification tree on the combined dataframe
tree_model <- rpart.control(maxdepth = 4, minsplit = 5, xval = 10, cp = 0)
tree <- rpart(Class~., data = combined_df, method = "class", control = tree_model)
#Plot tree
x11()
prp(tree, type = 4, extra = 101, leaf.round = 1, box.palette = "BuGn", nn=TRUE, main=" Classification Tree on combined dataset")
prp(tree, type = 4, extra = 101, leaf.round = 1, box.palette = "BuGn", nn=TRUE, main=" Classification Tree on combined dataset")
graphics.off()
#Plot tree
x11()
prp(tree, type = 4, extra = 101, leaf.round = 1, box.palette = "BuGn", nn=TRUE, main=" Classification Tree on combined dataset")
set.seed(907)
tree_model <- rpart.control(maxdepth = 4, minsplit = 5, xval = 10, cp = 0)
tree <- rpart(Class~., data = combined_df, method = "class", control = tree_model)
graphics.off()
#Plot tree
x11()
prp(tree, type = 4, extra = 101, leaf.round = 1, box.palette = "BuGn", nn=TRUE, main=" Classification Tree on combined dataset")
#Plot tree
x11()
prp(tree, type = 4, extra = 101, leaf.round = 1, box.palette = "BuGn", nn=TRUE, main=" Classification Tree on combined dataset")
###install and loard packages
install.packages('xgboost')
install.packages('stringr')
install.packages('caret')
install.packages('car')
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(car)
###Load datasets
load('dataset/preprocessed_dataset.Rdata')
load('dataset/diet_have.Rdata')
load('dataset/diet_nothave.Rdata')
raw_data = dataset
setwd(
'C:/Users/Administrator/iCloudDrive/Documents/UBSpring2020/CSE574/Project/Project'
)
###Load datasets
load('dataset/preprocessed_dataset.Rdata')
load('dataset/diet_have.Rdata')
load('dataset/diet_nothave.Rdata')
raw_data = dataset
#######Divide train and test
set.seed(123)
train_have <- sample(1:nrow(diet_have), .80*nrow(diet_have))
train_nothave <- sample(1:nrow(diet_nothave), .80*nrow(diet_nothave))
data_train <- rbind(diet_have[train_have, -c(2)], diet_nothave[train_nothave, -c(2)])
data_test <- rbind(diet_have[-train_have, -c(2)], diet_nothave[-train_nothave, -c(2)])
####### Over sampling
data.rose <- ROSE(diab ~ ., data = data_train, seed = 1)$data
library(ROSE)
####### Over sampling
data.rose <- ROSE(diab ~ ., data = data_train, seed = 1)$data
?xgboost
y_train <- as.numeric(data.rose$diab) - 1
y_train <- as.numeric(data.rose$diab)
y_train
table(y_train)
xgb <- xgboost(data = data_train, label = y_train)
m_data <- data.metrix(data.rose)
m_data <- data.rose.matrix
m_data <- data.matrix(data.rose, rownames.force = NA)
View(m_data)
#######Divide train and test
set.seed(123)
train_have <- sample(1:nrow(diet_have), .80*nrow(diet_have))
train_nothave <- sample(1:nrow(diet_nothave), .80*nrow(diet_nothave))
data_train <- rbind(diet_have[train_have, -c(2)], diet_nothave[train_nothave, -c(2)])
data_test <- rbind(diet_have[-train_have, -c(2)], diet_nothave[-train_nothave, -c(2)])
####### Over sampling
data.rose <- ROSE(diab ~ ., data = data_train, seed = 1)$data
data_train <- rbind(diet_have[train_have, -c(1,2)], diet_nothave[train_nothave, -c(1,2)])
data_test <- rbind(diet_have[-train_have, -c(1,2)], diet_nothave[-train_nothave, -c(1,2)])
####### Over sampling
data.rose <- ROSE(diab ~ ., data = data_train, seed = 1)$data
data_train <- rbind(diet_have[train_have, -c(2)], diet_nothave[train_nothave, -c(2)])
data_test <- rbind(diet_have[-train_have, -c(2)], diet_nothave[-train_nothave, -c(2)])
####### Over sampling
data.rose <- ROSE(diab ~ ., data = data_train, seed = 1)$data
m_data <- data.matrix(data.rose[, -c(1)], rownames.force = NA)
View(m_data)
m_data <- data.matrix(data.rose[, -c(1)], rownames.force = NA)
y_train <- as.numeric(data.rose$diab)
xgb <- xgboost(data = data_train, label = y_train)
xgb <- xgboost(data = m_data, label = y_train)
xgb <- xgboost(data = data.matrix(data.rose[, -c(1)]),
label = y_train,
eta = 0.1,
max_depth = 15,
nround=25,
subsample = 0.5,
colsample_bytree = 0.5,
seed = 1,
eval_metric = "merror",
objective = "multi:softprob",
num_class = 12,
nthread = 3
)
y_pred <- predict(xgb, data.matrix(data_test[, -c(1)]))
xgb <- xgboost(data = data.matrix(data.rose[, -c(1)]),
label = y_train,
eta = 0.3,
max_depth = 15,
nround = 25,
subsample = 0.5,
colsample_bytree = 0.5,
seed = 1,
eval_metric = "merror",
objective = "multi:softprob",
num_class = 12,
nthread = 3
)
y_pred <- predict(xgb, data.matrix(data_test[, -c(1)]))
mean(y_pred)
mean(y_pred)
for
y <- null
y <- null
y <- NULL
for(i in length(y_pred)){
if (y_pred[i] > y_mean){y[i] <- 1}
else {y[i] <- 0}
}
y_mean <- mean(y_pred)
y <- NULL
for(i in length(y_pred)){
if (y_pred[i] > y_mean){y[i] <- 1}
else {y[i] <- 0}
}
y_pred[i]
y_mean
y_pred <- predict(xgb, data.matrix(data_test[, -c(1)]))
y_mean <- mean(y_pred)
y <- NULL
data_test <- rbind(diet_have[-train_have, -c(2)], diet_nothave[-train_nothave, -c(2)])
importance <- xgb.importance(train_matrix@Dimnames[[2]], model = xgb)
importance <- xgb.importance(data.matrix(data.rose[, -c(1)]@Dimnames[[2]], model = xgb)
importance <- xgb.importance(data.matrix(data.rose[, -c(1)]@Dimnames[[2]], model = xgb)
importance <- xgb.importance(data.matrix(data.rose[, -c(1)]@Dimnames[[2]], model = xgb))
data.matrix(data.rose[, -c(1)]@Dimnames[[2]]
)
Dimnames
?Dimnames
importance <- xgb.importance(data.matrix(data.rose[, -c(1)], model = xgb))
importance <- xgb.importance(model = xgb))
importance <- xgb.importance(model = xgb))
importance <- xgb.importance(model = xgb)
head(importance)
xgb.ggplot.importance(importance)
Ckmeans.1d.dp
install.packages('Ckmeans.1d.dp')
library(Ckmeans.1d.dp)
xgb.ggplot.importance(importance)
importance <- xgb.importance(model = xgb)
head(importance)
xgb.ggplot.importance(importance)
pre_xgb = round(predict(xgb,newdata = data_test))
pre_xgb = predict(xgb,newdata = data.matrix(data_test[, -c(1)]))
pre_xgb = round(predict(xgb,newdata = data.matrix(data_test[, -c(1)])))
data.matrix(data_test[, -c(1)])
m_test <- data.matrix(data_test[, -c(1)])
View(m_test)
pre_xgb = round(predict(xgb, m_test)
)
pre_xgb = round(predict(xgb, m_test))
pre_xgb = round(predict(xgb, m_testâ€™))
pre_xgb = round(predict(xgb, t(m_test)))
pre_xgb = round(predict(xgb, m_test))
y_test <- as.numeric(data_test$diab)
table(y_test, pre_xgb)
rm(list = ls())
setwd(
'C:/Users/Administrator/iCloudDrive/Documents/UBSpring2020/CSE574/Project/Project'
)
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(car)
library(ROSE)
###Load datasets
load('dataset/preprocessed_dataset.Rdata')
load('dataset/diet_have.Rdata')
load('dataset/diet_nothave.Rdata')
raw_data = dataset
#######Divide train and test
set.seed(123)
train_have <- sample(1:nrow(diet_have), .80*nrow(diet_have))
train_nothave <- sample(1:nrow(diet_nothave), .80*nrow(diet_nothave))
data_train <- rbind(diet_have[train_have, -c(2)], diet_nothave[train_nothave, -c(2)])
data_test <- rbind(diet_have[-train_have, -c(2)], diet_nothave[-train_nothave, -c(2)])
y_test <- as.numeric(data_test$diab)
####### Over sampling
data.rose <- ROSE(diab ~ ., data = data_train, seed = 1)$data
m_data <- data.matrix(data.rose[, -c(1)], rownames.force = NA)
y_train <- as.numeric(data.rose$diab)
xgb <- xgboost(data = data.matrix(data.rose[, -c(1)]),
label = y_train,
eta = 0.3,
max_depth = 15,
nround = 25,
subsample = 0.5,
colsample_bytree = 0.5,
seed = 1,
eval_metric = "merror",
objective = "multi:softprob",
num_class = 12,
nthread = 3
)
xgboost(data = m_data, max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
xgboost(data = m_data,label = y_train, max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
importance <- xgb.importance(model = xgb)
head(importance)
xgb.ggplot.importance
head(importance)
xgb.ggplot.importance
graphics.off()
xgb.ggplot.importance
m_test <- data.matrix(data_test[, -c(1)])
pre_xgb = round(predict(xgb, m_test))
table(y_test, pre_xgb)
xgboost(data = m_data,label = y_train, max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
xgb <- xgboost(data = m_data,label = y_train, max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
importance <- xgb.importance(model = xgb)
head(importance)
graphics.off()
xgb.ggplot.importance
rm(list = ls())
setwd(
'C:/Users/Administrator/iCloudDrive/Documents/UBSpring2020/CSE574/Project/Project'
)
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(car)
library(ROSE)
###Load datasets
load('dataset/preprocessed_dataset.Rdata')
load('dataset/diet_have.Rdata')
load('dataset/diet_nothave.Rdata')
raw_data = dataset
#######Divide train and test
set.seed(123)
train_have <- sample(1:nrow(diet_have), .80*nrow(diet_have))
train_nothave <- sample(1:nrow(diet_nothave), .80*nrow(diet_nothave))
data_train <- rbind(diet_have[train_have, -c(2)], diet_nothave[train_nothave, -c(2)])
data_test <- rbind(diet_have[-train_have, -c(2)], diet_nothave[-train_nothave, -c(2)])
y_test <- as.numeric(data_test$diab)
####### Over sampling
data.rose <- ROSE(diab ~ ., data = data_train, seed = 1)$data
m_data <- data.matrix(data.rose[, -c(1)], rownames.force = NA)
y_train <- as.numeric(data.rose$diab)
xgb <- xgboost(data = m_data,label = y_train, max_depth=6, eta=0.5,
objective='binary:logistic', nround=25)
importance <- xgb.importance(model = xgb)
head(importance)
graphics.off()
xgb.ggplot.importance
importance <- xgb.importance(model = xgb)
head(importance)
graphics.off()
xgb.ggplot.importance(importance)
m_test <- data.matrix(data_test[, -c(1)])
pre_xgb = round(predict(xgb, m_test))
table(y_test, pre_xgb)
xgboost_roc <- roc(test_label,as.numeric(pre_xgb))
xgboost_roc <- roc(y_test, as.numeric(pre_xgb))
xgboost_roc <- roc(y_test, pre_xgb)
library(ROSE)
graphics.off()
xgb.ggplot.importance(importance)
m_test <- data.matrix(data_test[, -c(1)])
pre_xgb = round(predict(xgb, m_test))
table(y_test, pre_xgb)
xgboost_roc <- roc(y_test, pre_xgb)
xgboost_roc <- roc.curve(y_train, pre_xgb)
xgboost_roc <- roc.curve(y_test, pre_xgb)
?roc
xgboost_roc <- roc(y_test, pre_xgb)
install.packages('pROC')
install.packages("pROC")
library(pROC)
xgboost_roc <- roc(y_test, pre_xgb)
plot(xgboost_roc, print.auc=TRUE, auc.polygon=TRUE,
grid=c(0.1, 0.2),grid.col=c("green", "red"),
max.auc.polygon=TRUE,auc.polygon.col="skyblue",
print.thres=TRUE,main='ROC curve')
